# Entropy (‡¶è‡¶®‡¶ü‡ßç‡¶∞‡¶™‡¶ø) ‚Äì Decision Tree-‡¶è‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶Æ‡ßå‡¶≤‡¶ø‡¶ï ‡¶ß‡¶æ‡¶∞‡¶£‡¶æ

Machine Learning-‡¶è, ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑ ‡¶ï‡¶∞‡ßá Decision Tree ‡¶Ö‡ßç‡¶Ø‡¶æ‡¶≤‡¶ó‡¶∞‡¶ø‡¶¶‡¶Æ‡ßá, **Entropy** ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ó‡ßÅ‡¶∞‡ßÅ‡¶§‡ßç‡¶¨‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶ß‡¶æ‡¶∞‡¶£‡¶æ‡•§

‡¶∏‡¶π‡¶ú ‡¶≠‡¶æ‡¶∑‡¶æ‡ßü:

> Entropy ‡¶π‡¶≤‡ßã ‡¶ï‡ßã‡¶®‡ßã ‡¶è‡¶ï‡¶ü‡¶ø node-‡¶è ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶Ö‡¶®‡¶ø‡¶∂‡ßç‡¶ö‡ßü‡¶§‡¶æ (uncertainty) ‡¶¨‡¶æ impurity ‡¶™‡¶∞‡¶ø‡¶Æ‡¶æ‡¶™‡ßá‡¶∞ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶â‡¶™‡¶æ‡ßü‡•§

* ‡¶∏‡¶¨ sample ‡¶è‡¶ï‡¶á class ‡¶π‡¶≤‡ßá ‚Üí Entropy = 0 (Pure Node)
* ‡¶∏‡¶¨ class ‡¶∏‡¶Æ‡¶æ‡¶®‡¶≠‡¶æ‡¶¨‡ßá ‡¶Æ‡¶ø‡¶∂‡ßç‡¶∞‡¶ø‡¶§ ‡¶π‡¶≤‡ßá ‚Üí Entropy ‡¶∏‡¶∞‡ßç‡¶¨‡ßã‡¶ö‡ßç‡¶ö

Entropy ‡¶ß‡¶æ‡¶∞‡¶£‡¶æ‡¶ü‡¶ø Information Theory ‡¶•‡ßá‡¶ï‡ßá ‡¶è‡¶∏‡ßá‡¶õ‡ßá‡•§

---

## ‡¶ó‡¶æ‡¶£‡¶ø‡¶§‡¶ø‡¶ï ‡¶∏‡¶Ç‡¶ú‡ßç‡¶û‡¶æ

‡¶ß‡¶∞‡¶ø, ‡¶ï‡ßã‡¶®‡ßã dataset-‡¶è ‡¶Æ‡ßã‡¶ü **C** ‡¶ü‡¶ø class ‡¶Ü‡¶õ‡ßá‡•§ ‡¶§‡¶æ‡¶π‡¶≤‡ßá entropy ‡¶∏‡¶Ç‡¶ú‡ßç‡¶û‡¶æ‡ßü‡¶ø‡¶§ ‡¶π‡ßü:

$$
H = - \sum_{i=1}^{C} p_i \log_2(p_i)
$$

‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá:

* $p_i$ = class i-‡¶è‡¶∞ probability
* $C$ = ‡¶Æ‡ßã‡¶ü class ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ

Convention ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡ßü‡ßÄ:

$$
0 \log_2(0) = 0
$$

‡¶ï‡¶æ‡¶∞‡¶£,

$$
\lim_{p \to 0} p \log(p) = 0
$$

Negative ‡¶ö‡¶ø‡¶π‡ßç‡¶® ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü ‡¶ï‡¶æ‡¶∞‡¶£ $0 < p_i < 1$ ‡¶π‡¶≤‡ßá $\log(p_i)$ negative ‡¶π‡ßü‡•§

---

## Information ‡¶¨‡¶æ Surprise ‡¶ß‡¶æ‡¶∞‡¶£‡¶æ

Information Theory ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡ßü‡ßÄ ‡¶ï‡ßã‡¶®‡ßã ‡¶ò‡¶ü‡¶®‡¶æ‡¶∞ information:

$$
I(p) = -\log_2(p)
$$

* Rare ‡¶ò‡¶ü‡¶®‡¶æ ‚Üí ‡¶¨‡ßá‡¶∂‡¶ø information
* Common ‡¶ò‡¶ü‡¶®‡¶æ ‚Üí ‡¶ï‡¶Æ information

Entropy ‡¶π‡¶≤‡ßã ‡¶è‡¶á information-‡¶è‡¶∞ ‡¶ó‡ßú ‡¶Æ‡¶æ‡¶®:

$$
H = \mathbb{E}[I(p)]
$$

‡¶Ö‡¶∞‡ßç‡¶•‡¶æ‡ßé, distribution ‡¶•‡ßá‡¶ï‡ßá random ‡¶è‡¶ï‡¶ü‡¶ø sample ‡¶®‡¶ø‡¶≤‡ßá ‡¶ó‡ßú‡ßá ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶ï‡¶§‡¶ü‡¶æ "‡¶Ö‡¶¨‡¶æ‡¶ï" ‡¶π‡¶¨‡ßã‡•§

---

## Binary Classification ‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡ßá

‡¶¶‡ßÅ‡¶ü‡¶ø class ‡¶•‡¶æ‡¶ï‡¶≤‡ßá probability ‡¶ß‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶ï $(p, 1-p)$‡•§

$$
H(p) = -p \log_2(p) - (1-p) \log_2(1-p)
$$

### ‡¶ï‡ßá‡¶∏ ‡ßß: ‡¶∏‡¶Æ‡ßç‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶¨‡¶ø‡¶∂‡ßÅ‡¶¶‡ßç‡¶ß

‡¶Ø‡¶¶‡¶ø $p = 1$:

$$
H = -1 \log_2(1) = 0
$$

### ‡¶ï‡ßá‡¶∏ ‡ß®: ‡ß´‡ß¶%-‡ß´‡ß¶% ‡¶¨‡¶ø‡¶≠‡¶æ‡¶ú‡¶®

‡¶Ø‡¶¶‡¶ø $p = 0.5$:

$$
H = -0.5 \log_2(0.5) - 0.5 \log_2(0.5)
$$

‡¶Ø‡ßá‡¶π‡ßá‡¶§‡ßÅ:

$$
\log_2(0.5) = -1
$$

‡¶§‡¶æ‡¶á:

$$
H = 1
$$

‡¶è‡¶ü‡¶ø binary entropy-‡¶è‡¶∞ ‡¶∏‡¶∞‡ßç‡¶¨‡ßã‡¶ö‡ßç‡¶ö ‡¶Æ‡¶æ‡¶®‡•§

### ‡¶ï‡ßá‡¶∏ ‡ß©: ‡ß≠‡ß¶%-‡ß©‡ß¶%

$$
H = -0.7 \log_2(0.7) - 0.3 \log_2(0.3)
$$

$$
H \approx 0.88
$$

---

## Multi-Class ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£

‡¶ß‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶ï ‡¶è‡¶ï‡¶ü‡¶ø node-‡¶è 100‡¶ü‡¶ø sample:

* Class A: 50
* Class B: 30
* Class C: 20

Probability:

$$
p_A = 0.5, \quad p_B = 0.3, \quad p_C = 0.2
$$

Entropy:

$$
H = -0.5 \log_2(0.5) - 0.3 \log_2(0.3) - 0.2 \log_2(0.2)
$$

$$
H \approx 1.485
$$

‡¶∏‡¶∞‡ßç‡¶¨‡ßã‡¶ö‡ßç‡¶ö entropy (‡ß©‡¶ü‡¶ø class ‡¶π‡¶≤‡ßá):

$$
H_{max} = \log_2(3) \approx 1.585
$$

---

## Entropy-‡¶è‡¶∞ ‡¶∏‡ßÄ‡¶Æ‡¶æ (Bounds)

‡¶∏‡¶∞‡ßç‡¶¨‡¶®‡¶ø‡¶Æ‡ßç‡¶®:

$$
H_{min} = 0
$$

‡¶∏‡¶∞‡ßç‡¶¨‡ßã‡¶ö‡ßç‡¶ö:

$$
H_{max} = \log_2(C)
$$

| Class ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ | ‡¶∏‡¶∞‡ßç‡¶¨‡ßã‡¶ö‡ßç‡¶ö Entropy |
| ------------ | ---------------- |
| 2            | 1                |
| 3            | 1.585            |
| 4            | 2                |
| 10           | 3.322            |

---

## Decision Tree-‡¶§‡ßá Entropy

Decision Tree-‡¶è‡¶∞ ‡¶≤‡¶ï‡ßç‡¶∑‡ßç‡¶Ø ‡¶π‡¶≤‡ßã impurity ‡¶ï‡¶Æ‡¶æ‡¶®‡ßã‡•§

‡¶è‡¶á ‡¶ï‡¶Æ‡¶æ‡¶®‡ßã‡¶∞ ‡¶™‡¶∞‡¶ø‡¶Æ‡¶æ‡¶£‡¶ï‡ßá ‡¶¨‡¶≤‡¶æ ‡¶π‡ßü **Information Gain (IG)**:

$$
IG = H(parent) - \sum_{k} \frac{n_k}{n} H(child_k)
$$

‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá:

* $n$ = parent node-‡¶è‡¶∞ ‡¶Æ‡ßã‡¶ü sample
* $n_k$ = child k-‡¶è‡¶∞ sample ‡¶∏‡¶Ç‡¶ñ‡ßç‡¶Ø‡¶æ
* $H(child_k)$ = child node-‡¶è‡¶∞ entropy

‡¶Ø‡ßá split ‡¶∏‡¶∞‡ßç‡¶¨‡ßã‡¶ö‡ßç‡¶ö Information Gain ‡¶¶‡ßá‡ßü, ‡¶∏‡ßá‡¶ü‡¶ø‡¶á ‡¶®‡¶ø‡¶∞‡ßç‡¶¨‡¶æ‡¶ö‡¶® ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§

---

## ‡¶ß‡¶æ‡¶™‡ßá ‡¶ß‡¶æ‡¶™‡ßá ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£

‡¶ß‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶ï label:

```
[A, A, B, A, B, B, A, C, A, C]
```

Class count:

* A = 5
* B = 3
* C = 2

Probability:

$$
p_A = 0.5, \quad p_B = 0.3, \quad p_C = 0.2
$$

Entropy:

$$
H = 1.485
$$

---

## Binary Entropy Curve ‡¶¨‡ßà‡¶∂‡¶ø‡¶∑‡ßç‡¶ü‡ßç‡¶Ø

* $H(0) = 0$
* $H(0.5) = 1$ (Maximum)
* $H(1) = 0$

Curve symmetric ‡¶è‡¶¨‡¶Ç concave‡•§

---

## ‡¶ï‡ßá‡¶® Log Base 2?

Log base 2 ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶≤‡ßá entropy ‡¶Æ‡¶æ‡¶™‡¶æ ‡¶π‡ßü **bits**-‡¶è‡•§

* 1 bit ‚Üí 1‡¶ü‡¶ø yes/no ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®
* 2 bits ‚Üí 2‡¶ü‡¶ø ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®

Base ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶® ‡¶ï‡¶∞‡¶≤‡ßá scale ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶ø‡¶§ ‡¶π‡ßü, ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶§‡ßÅ‡¶≤‡¶®‡¶æ‡¶Æ‡ßÇ‡¶≤‡¶ï ‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶£ ‡¶è‡¶ï‡¶á ‡¶•‡¶æ‡¶ï‡ßá‡•§

---

## Entropy ‡¶¨‡¶®‡¶æ‡¶Æ Gini

| Entropy         | Gini               |
| --------------- | ------------------ |
| Log ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá | Log ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶®‡¶æ |
| ‡¶∏‡¶æ‡¶Æ‡¶æ‡¶®‡ßç‡¶Ø ‡¶ß‡ßÄ‡¶∞     | ‡¶¶‡ßç‡¶∞‡ßÅ‡¶§              |
| ‡¶¨‡ßá‡¶∂‡¶ø sensitive  | ‡¶ï‡¶ø‡¶õ‡ßÅ‡¶ü‡¶æ smooth      |

‡¶Ö‡¶®‡ßá‡¶ï implementation-‡¶è (‡¶Ø‡ßá‡¶Æ‡¶® scikit-learn) default ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá Gini ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§ ‡¶π‡ßü‡•§

---

## üîó Cross-Entropy ‡¶∏‡¶Ç‡¶Ø‡ßã‡¶ó

Neural Network-‡¶è ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡ßÉ‡¶§ loss:

$$
CE = - \sum_i y_i \log(\hat{p}_i)
$$

‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá:

* $y_i$ = true label (one-hot)
* $\hat{p}_i$ = predicted probability

Shannon entropy ‡¶π‡¶≤‡ßã cross-entropy-‡¶è‡¶∞ ‡¶¨‡¶ø‡¶∂‡ßá‡¶∑ ‡¶∞‡ßÇ‡¶™ ‡¶Ø‡¶ñ‡¶®:

$$
\hat{p}_i = p_i
$$



